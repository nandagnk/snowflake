======================
Snowflake Architecture
======================
Reference : https://docs.snowflake.com/en/user-guide/intro-key-concepts

1. Cloud Services Layer
2. Query Proecessing Layer
3. Data Storage Layer - Hybird Columnar Storage - Compressed and saved as blobs in Cloud storage.

1. Cloud Services Layer
=======================
Authentication, Access Control
Security
Infrastructure manager  -- creation of warehouse,users, roles, any objects - no need of warehouse to perform these operations, managed internally.
Matadata manager - Metadata management 
Query Optimizer - Query Optimization
Query Planning
Refered as brain of the snowflake system

2. Query Processing Layer - Alternatively referred as compute layer, Compute Notes, Virtual warehouse, cloud computing
=========================
perform actual query processing.
Data Loading, Unloading, Data Processing, DML etc.
Performs MMP (Masive parallel processing)
Refered as muscle of the snowflake system.

3. Data Storage Layer
======================
Data is stored in Hybird Columnar Storage (data is stored in columns) - Compressed and saved as blobs in Cloud storage. More efficient in storing and 
performing analytical data processing.

Warehouse Sizes (Billed per seconds, minimum 60 seconds)

XSmall 					Consists 1 server 				- consumes 1 Credit for 1 hour
Small					consists 2 servers				- consumes 2 Credits for 1 hour
Medium					consists 4 servers				- consumes 4 Credits for 1 hour
Large					consists 8 servers				- consumes 8 Credits for 1 hour
XLarge					consists 16 servers				- consumes 16 Credits for 1 hour
2XLarge					consists 32 servers				- consumes 32 Credits for 1 hour
3XLarge					consists 64 servers				- consumes 64 Credits for 1 hour
4XLarge					consists 128 servers				- consumes 128 Credits for 1 hour
5XLarge					consists 256 servers				- consumes 256 Credits for 1 hour

Quiz

Question 1:
What are the three layers of Snowflake?
Answer : Storage, Query Processing, Cloud Services.
Question 2:
What layer is referred to as "muscle of the system"?
Answer : Query Processing
Question 3:
What layer is often referred to as "brain of the system"?
Answer : Cloud Services Layer
Question 4:
We use the virtual warehouses as compute resources to process queries. True or false?
Answer : True
Question 5:
A virtual warehouse of the size M has ... how many servers running to process queries?
Answer : 4 Servers
================================================================================================================================
End of Topic - Snowflake Architecture
================================================================================================================================

======================
Setting up a warehouse
======================

1. using interface
2. using sql command

Reference: https://docs.snowflake.com/en/user-guide/warehouses

AccountAdmin or Security Admin can view the existing warehouses.
Only AccountAdmin and Sysadmin or other custom roles as accountadmin/sysadmin access only can create a new warehouse.

Types - Standard / Snowpark-Optimized
Standard - used for most workloads including snowpark. 
Snowpark-Optimized - offers a larger memory and cache. They are often used for memory intensive operations. Machine Learning an example.

Multicluster warehouse is a group of warehouses of the samesize, multicluster will have the minimum and maximum number of clusters defined.
Minimum - is when the load is less or normal
and maximum will get scaled out when the load increases 
Multiclustering is to support concurrent processing of queries.

Multicluster scalling policy  - Standard or Economy 
Standard(default) will spin-up the additional cluster once the active clusters are busy and there is a demand. This prevents/minimizes the queuing of workloads. And
shutdown the cluster after 2 to 3 consecutive successful checks(performed 1 minute interval) resulting the load is less and cluster is inactive.
Economy : Conserves credits by keeping the running clusters fully loaded than starting additional node. This may result in queries being queued and taking longer 
to complete. This will spin-up the additional cluster when there is a demand to keep the server busy for atleast 6 minutes and shutdown the node after 5 to 6 consecutive 
successful checks resulting the load is less and cluster is inactive. 

When to go for multicluster?
1. when Number of users increses in the system.
2. when number of queries/workloads increases to get waiting in the queue.

Query Acceleration
Accelerate outlier queries with additional flexible compute resources. 
Learn more - https://docs.snowflake.com/en/user-guide/query-acceleration-service

How to size the warehouse?
The best pracise is to start with XtraSmall and run various queries/load and determine the correct size.

Other best pracises in setting up a warehouse.
Setup the Autoresume parameter true and define the time limit for the parameter Autosuspend (default 10 minutes )


Setting up a warehouse using sql command

use role accountadmin; or 
use role sysadmin;

create or replace warehouse <warehouse_name>
with
warehouse_size = 'xsmall'
min_cluster_count = 1
max_cluster_count = 3
auto_resume = True
comment = ''

example

create or replace warehouse Finanace_wh
with
warehouse_size = 'xsmall'
min_cluster_count = 1
max_cluster_count = 3
auto_resume = True
initially_suspended = true
comment = 'This warehouse used by finanace team for various workloads related to finance.';

Warehouse FINANACE_WH successfully created.

show warehouses;

name       |state    |type    |size   |min_cluster_count|max_cluster_count|started_clusters|running|queued|is_default|is_current|auto_suspend|auto_resume|available|provisioning|quiescing|other|created_on             |resumed_on             |updated_on             |owner       |comment                                                                       |enable_query_acceleration|query_acceleration_max_scale_factor|resource_monitor|actives|pendings|failed|suspended|uuid    |scaling_policy|budget|owner_role_type|
-----------+---------+--------+-------+-----------------+-----------------+----------------+-------+------+----------+----------+------------+-----------+---------+------------+---------+-----+-----------------------+-----------------------+-----------------------+------------+------------------------------------------------------------------------------+-------------------------+-----------------------------------+----------------+-------+--------+------+---------+--------+--------------+------+---------------+
COMPUTE_WH |SUSPENDED|STANDARD|X-Small|                1|                1|               0|      0|     0|Y         |Y         |         600|true       |         |            |         |     |2024-05-15 11:09:33.284|2024-06-12 23:48:00.437|2024-06-12 23:48:00.437|ACCOUNTADMIN|                                                                              |false                    |                                  8|null            |      0|       0|     0|        1|29898244|STANDARD      |      |ROLE           |
FINANACE_WH|SUSPENDED|STANDARD|X-Small|                1|                3|               0|      0|     0|N         |N         |         600|true       |         |            |         |     |2024-06-23 00:06:30.719|2024-06-23 00:06:30.729|2024-06-23 00:06:30.752|SYSADMIN    |This warehouse used by finanace team for various workloads related to finance.|false                    |                                  8|null            |      0|       0|     0|        1|29898268|STANDARD      |      |ROLE           |

SHOW PARAMETERS for warehouse FINANACE_WH;
key                                |value |default|level|description                                                                                                                                                                        |type  |
-----------------------------------+------+-------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+
MAX_CONCURRENCY_LEVEL              |8     |8      |     |Maximum number of SQL statements a warehouse cluster can execute concurrently before queuing them. Small SQL statements count as a fraction of 1.                                  |NUMBER|
STATEMENT_QUEUED_TIMEOUT_IN_SECONDS|0     |0      |     |Timeout in seconds for queued statements: statements will automatically be canceled if they are queued on a warehouse for longer than this amount of time; disabled if set to zero.|NUMBER|
STATEMENT_TIMEOUT_IN_SECONDS       |172800|172800 |     |Timeout in seconds for statements: statements are automatically canceled if they run for longer; if set to zero, max value (604800) is enforced.                                   |NUMBER|

Assignment - 1
==============

1. To be able to create the virtual warehouse, you have to use at least the role SYSADMIN (or SECURITYADMIN or ACCOUNTADMIN).

USE ROLE SYSADMIN;


2. Set up a virtual warehouse using SQL command:

CREATE WAREHOUSE EXERCISE_WH
WAREHOUSE_SIZE = XSMALL
AUTO_SUSPEND = 600  -- automatically suspend the virtual warehouse after 10 minutes of not being used
AUTO_RESUME = TRUE 
COMMENT = 'This is a virtual warehouse of size X-SMALL that can be used to process queries.';


3. Drop the virtual warehouse

DROP WAREHOUSE EXERCISE_WH;


Questions for this assignment
After how many minutes will this virtual warehouse be suspended if it is not being used?
Answer 10 minutes

================================================================================================================================
End of Topic - Setting up a warehouse
================================================================================================================================

=================
Manage warehouses
=================
To change the warehouse we use alter command

alter warehouse <warehouse_name>
set parameter_name = value

alter warehouse Finanace_wh
set 
max_cluster_count = 1,  //changing the warehouse from multicluse warehouse to non-multicluster warehouse
auto_suspend = 120,      //suspend the server when it is inactive for 120 seconds
warehouse_size = 'small' //scale up the warehouse from xtrasmall to small.

statement executed successfully.

show WAREHOUSES LIKE 'Finanace_wh';

name       |state    |type    |size |min_cluster_count|max_cluster_count|started_clusters|running|queued|is_default|is_current|auto_suspend|auto_resume|available|provisioning|quiescing|other|created_on             |resumed_on             |updated_on             |owner   |comment                                                                       |enable_query_acceleration|query_acceleration_max_scale_factor|resource_monitor|actives|pendings|failed|suspended|uuid    |scaling_policy|budget|owner_role_type|
-----------+---------+--------+-----+-----------------+-----------------+----------------+-------+------+----------+----------+------------+-----------+---------+------------+---------+-----+-----------------------+-----------------------+-----------------------+--------+------------------------------------------------------------------------------+-------------------------+-----------------------------------+----------------+-------+--------+------+---------+--------+--------------+------+---------------+
FINANACE_WH|SUSPENDED|STANDARD|Small|                1|                1|               0|      0|     0|N         |N         |         120|true       |         |            |         |     |2024-06-23 00:06:30.719|2024-06-23 00:06:30.729|2024-06-23 00:21:52.742|SYSADMIN|This warehouse used by finanace team for various workloads related to finance.|false                    |                                  8|null            |      0|       0|     0|        2|29898268|STANDARD      |      |ROLE           |

Drop the warehouse
==================
Only the owner of the warehouse or accountadmin/sysadmin can drop the warehouse.
drop warehouse <warehouse_name>;

drop warehouse Finanace_wh;

================================================================================================================================
End of Topic - Managing warehouses
================================================================================================================================

==============================
Exploring Databases and tables
==============================

USE ROLE sysadmin;

//creating database

CREATE DATABASE OUR_FIRST_DB;

//Creating the table

CREATE TABLE "OUR_FIRST_DB"."PUBLIC"."LOAN_PAYMENT" (
  "Loan_ID" STRING,
  "loan_status" STRING,
  "Principal" STRING,
  "terms" STRING,
  "effective_date" STRING,
  "due_date" STRING,
  "paid_off_time" STRING,
  "past_due_days" STRING,
  "age" STRING,
  "education" STRING,
  "Gender" STRING);
  
//Check that table is empy
USE DATABASE OUR_FIRST_DB;

SELECT count(1) rec_count FROM OUR_FIRST_DB.PUBLIC.LOAN_PAYMENT;

REC_COUNT|
---------+
        0|
		
//Loading the data from S3 bucket
  
COPY INTO LOAN_PAYMENT
   FROM s3://bucketsnowflakes3/Loan_payments_data.csv
   file_format = (type = csv 
                  field_delimiter = ',' 
                  skip_header=1);
				  
 //Loading the data from S3 bucket
  
Updated Rows	500
Query	 //Loading the data from S3 bucket
	 COPY INTO LOAN_PAYMENT
	    FROM s3://bucketsnowflakes3/Loan_payments_data.csv
	    file_format = (type = csv 
	                   field_delimiter = ',' 
	                   skip_header=1)
Start time	Sun Jun 23 00:55:29 AEST 2024
Finish time	Sun Jun 23 00:55:32 AEST 2024

SELECT count(1) rec_count  FROM OUR_FIRST_DB.PUBLIC.LOAN_PAYMENT;
REC_COUNT|
---------+
      500|
	  
Assignment 2 - Load Data
========================


1. Create a database called EXERCISE_DB

use role sysadmin;

create database if not exists exercise_db;


2. Create a table called CUSTOMERS

Set the column names and data types as follows:

ID INT,

first_name varchar,

last_name varchar,

email varchar,

age int,

city varchar

use exercise_db;
create table if not exists customers(
ID INT,
first_name varchar,
last_name varchar,
email varchar,
age int,
city varchar);

3. Load the data in the table

The data is available under: s3://snowflake-assignments-mc/gettingstarted/customers.csv.
Data type: CSV - delimited by ',' (comma)
Header is in the first line.

copy into customers
from s3://snowflake-assignments-mc/gettingstarted/customers.csv
file_format = (type = csv
               field_delimiter = ','
			   skip_header = 1);
			   
file	status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflake-assignments-mc/gettingstarted/customers.csv	LOADED	1000	1000	1	0				

4. Query from that table. How many rows are now in the table?

select count(1) row_count from exercise_db.public.customers;

REC_COUNT|
---------+
     1000|

Questions for this assignment
How many rows are within the table after the load?
Answer: 1000 rows

================================================================================================================================
End of Topic - Exploring Databases and Tables
================================================================================================================================

==================
Snowflake editions
==================
1. Standard 
2. Enterprises = Standard + additional features
3. Business Critical = Enterprises + additional features
4. Virtual Private =  Business Critical + additional features - This uses dedicate environment and dedicated features 


Standard edition features
=========================
Time travel is 1 day for permanent tables.
Disaster Recovery or Fail safe is 7 days for permanent tables.
Network policies
secure data share
Federated Athentication and SSO
Premier support 24/7

Enterprises edition features
============================
All Standard edition features +
Time travel upto 90 days
Materialised views
Search Optimization
column level security
24 hours early access to weekly new releases.

Business Critical edition features
==================================
All Enterprises edition features +
Customer managed encryption
support for data specific regulations
Seamless database recovery failover/failback 

Virtual Private edition features
================================
All Business Critical edition features +
Dedicated virtual servers and
completely separate snowflake environment.

================================================================================================================================
End of Topic - Snowflake editions
================================================================================================================================

==================
Roles in Snowflake
==================

SYSTEM DEFINED Roles, These roles can't be dropped.
 
ORGADMIN    	---> manages actions on Organization level, create accounts, view all accounts , view account usage information
ACCOUNTADMIN  	---> Top level in the hierarchy contains lots of previlages, monitors the account,
SECURITYADMIN  	---> create and monitors and manage security part.
USERADMIN     	---> creates and manages users and roles and previlages
SYSADMIN      	---> creates and manages warehouses, databases, schemas, tables, views, and other db objects and manage the objects, grant previlages	on these objects to other roles 
PUBLIC       	---> default role assgined to every users once created.

CUSTOM Roles

Assgn the custom rolse to SYSADMIN / Security Admin based on the actions to be carried out by these custom roles. Otherwise these roles cannot be managed by Security Admin or Sysadmin roles.

Quiz 2
======
Question 1:
What are the different Snowflake editions?
Answer : Standard, Enterprises, Business Critical and Virtual Private.
Question 2:
If we require the highest data protection & security. Which one would we rather choose?
Answer : Virtual Private 
Question 3:
What is mainly relevant in pricing/costs of snowflake?
Answer: Storage and Compute resources
Question 4:
What role is the "top-level" role with most privileges?
Answer : AccountAdmin
==================================================

================================================================================================================================
End of Topic - Roles in Snowflake
================================================================================================================================

============
Loading Data
============
Types of loading

a. Bulk loading
	Most frequent method
	Uses warehouses
	Loading from stages
	COPY command
	Transformations possible 	

b. Continuous loading
	Designed to load small volumes of data
	Automatically once they are added to stages
	Lates results for analysis
	Snowpipe (Serverless feature)

Types of Stages
a. Internal stages (Local storage provided by Snowflake)
	Named Stages
	User Stages
	Table Stages

External Stages
	External storages (GCP, Azure Blob storge or Amazson S3)
	Requires a data integration object (for secured data loading)
	Requires a Stage URLs

Creating an external stage
==========================

use role sysadmin;

CREATE OR REPLACE DATABASE MANAGE_DB;

CREATE OR REPLACE SCHEMA external_stages;

// Publicly accessible staging area 
CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage
    url='s3://bucketsnowflakes3';
	
//SHOW Stage

show stages;
created_on             |name     |database_name|schema_name    |url                   |has_credentials|has_encryption_key|owner   |comment|region   |type    |cloud|notification_channel|storage_integration|endpoint|owner_role_type|directory_enabled|
-----------------------+---------+-------------+---------------+----------------------+---------------+------------------+--------+-------+---------+--------+-----+--------------------+-------------------+--------+---------------+-----------------+
2024-06-24 17:49:01.987|AWS_STAGE|MANAGE_DB    |EXTERNAL_STAGES|s3://bucketsnowflakes3|N              |N                 |SYSADMIN|       |us-east-1|EXTERNAL|AWS  |                    |                   |        |ROLE           |N                |

// List files in stage

LIST @aws_stage;
name                                         |size |md5                             |last_modified               |
---------------------------------------------+-----+--------------------------------+----------------------------+
s3://bucketsnowflakes3/Loan_payments_data.csv|44417|f354864a37b2dbec495190058d427285|Mon, 5 Apr 2021 12:28:12 GMT|
s3://bucketsnowflakes3/OrderDetails.csv      |54600|1a1c4a47a8e8e43ecef5bf8a46ee4017|Tue, 6 Apr 2021 16:42:56 GMT|
s3://bucketsnowflakes3/sampledata.csv        |  158|462259b257e0238ff9f9fb08313c2637|Tue, 6 Apr 2021 16:56:32 GMT|

//create table Orders

CREATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS(
ORDER_ID VARCHAR(30),
AMOUNT INT,
PROFIT INT,
QUANTITY INT,
CATEGORY VARCHAR(30),
SUBCATEGORY VARCHAR(30));

//Load data using copy into <table> command
reference : https://docs.snowflake.com/en/sql-reference/sql/copy-into-table

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
	
select count(1) from OUR_FIRST_DB.PUBLIC.ORDERS;

COUNT(1)|
--------+
    1500|
	
// Copy command with fully qualified stage object

truncate table OUR_FIRST_DB.PUBLIC.ORDERS;

//This will fail as there are multiple files with different columns there in stage and in the below copy command file_name or patter is not specified
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1);
	
SQL Error [100080] [22000]: Number of columns in file (2) does not match that of the corresponding table (6), 
    use file format option error_on_column_count_mismatch=false to ignore this error
  File 'sampledata.csv', line 3, character 1
  Row 1 starts at line 2, column "ORDERS"["AMOUNT":2]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' 
  or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client.
  
now load the specific file "OrderDetails.csv", we can specifiy multiple files also in a comma separated list
  
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails.csv');
    
select count(1) from OUR_FIRST_DB.PUBLIC.ORDERS;

COUNT(1)|
--------+
    1500|

Run the copy command once again and check the count, there won't be any change. 
Since the file "OrderDetails.csv" is already loaded snowflake will not process the file again.

now truncate the table again and reload the same file using patter keyword, when the table is truncated, previous load related metadata is removed as a result 
snowflake allow to load the data from the same file again.

truncate table OUR_FIRST_DB.PUBLIC.ORDERS;

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS
    FROM @MANAGE_DB.external_stages.aws_stage
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
    
select count(1) from OUR_FIRST_DB.PUBLIC.ORDERS;

COUNT(1)|
--------+
    1500|
	
How to query the data directly from the external stage files without loading to tables? or before loading to tables?

step 1 - create a file format
step 2 - query the data using the fileformat.

CREATE OR REPLACE FILE FORMAT my_csv_format_1
TYPE = 'csv' FIELD_DELIMITER = ',' skip_header = 1;

select
	$1::string AS orderid,
	$2::int AS amount,
	$3::int AS profit,
	$4::int AS quantity,
	$5::STRING AS category ,
	$6::STRING AS subcategory
from @MANAGE_DB.external_stages.aws_stage 
(file_format => my_csv_format_1, pattern=>'.*Order.*')
limit 10;

ORDERID|AMOUNT|PROFIT|QUANTITY|CATEGORY   |SUBCATEGORY     |
-------+------+------+--------+-----------+----------------+
B-25601|  1275| -1148|       7|Furniture  |Bookcases       |
B-25601|    66|   -12|       5|Clothing   |Stole           |
B-25601|     8|    -2|       3|Clothing   |Hankerchief     |
B-25601|    80|   -56|       4|Electronics|Electronic Games|
B-25602|   168|  -111|       2|Electronics|Phones          |
B-25602|   424|  -272|       5|Electronics|Phones          |
B-25602|  2617|  1151|       4|Electronics|Phones          |
B-25602|   561|   212|       3|Clothing   |Saree           |
B-25602|   119|    -5|       8|Clothing   |Saree           |
B-25603|  1355|   -60|       5|Clothing   |Trousers        |

we can perform transformations on the data now.

select
	$1::string AS orderid,
	$2::int AS amount,
	case when cast($3 AS int) <0 then 'Non-Profitable' else 'Profitable' end AS Opinion,
	$4::int AS quantity,
	$5::STRING AS category ,
	$6::STRING AS subcategory
from @MANAGE_DB.external_stages.aws_stage 
(file_format => my_csv_format_1, pattern=>'.*Order.*')
limit 10;
ORDERID|AMOUNT|OPINION       |QUANTITY|CATEGORY   |SUBCATEGORY     |
-------+------+--------------+--------+-----------+----------------+
B-25601|  1275|Non-Profitable|       7|Furniture  |Bookcases       |
B-25601|    66|Non-Profitable|       5|Clothing   |Stole           |
B-25601|     8|Non-Profitable|       3|Clothing   |Hankerchief     |
B-25601|    80|Non-Profitable|       4|Electronics|Electronic Games|
B-25602|   168|Non-Profitable|       2|Electronics|Phones          |
B-25602|   424|Non-Profitable|       5|Electronics|Phones          |
B-25602|  2617|Profitable    |       4|Electronics|Phones          |
B-25602|   561|Profitable    |       3|Clothing   |Saree           |
B-25602|   119|Non-Profitable|       8|Clothing   |Saree           |
B-25603|  1355|Non-Profitable|       5|Clothing   |Trousers        |


//table has 4 columns but we are going to populate only two columns

CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX (
    ORDER_ID VARCHAR(30),
    AMOUNT INT,
    PROFIT INT,
    PROFITABLE_FLAG VARCHAR(30)  
    );

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX (ORDER_ID,PROFIT)
    FROM (select 
            s.$1,
            s.$3
          from @MANAGE_DB.external_stages.aws_stage s)
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files=('OrderDetails.csv');
	
SELECT *
FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX
LIMIT 10;
	
ORDER_ID|AMOUNT|PROFIT|PROFITABLE_FLAG|
--------+------+------+---------------+
B-25601 |      | -1148|               |
B-25601 |      |   -12|               |
B-25601 |      |    -2|               |
B-25601 |      |   -56|               |
B-25602 |      |  -111|               |
B-25602 |      |  -272|               |
B-25602 |      |  1151|               |
B-25602 |      |   212|               |
B-25602 |      |    -5|               |
B-25603 |      |   -60|               |


//Example 5 - Table Auto increment

CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX (
    ORDER_ID number autoincrement start 1 increment 1,
    AMOUNT INT,
    PROFIT INT,
    PROFITABLE_FLAG VARCHAR(30)  
    );
	

//Example 5 - Auto increment ID

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX (PROFIT,AMOUNT)
    FROM (select 
            s.$2,
            s.$3
          from @MANAGE_DB.external_stages.aws_stage s)
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files=('OrderDetails.csv');
	
file	status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes3/OrderDetails.csv	LOADED	1500	1500	1	0				


SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX WHERE ORDER_ID > 15
LIMIT 10;

ORDER_ID|AMOUNT|PROFIT|PROFITABLE_FLAG|
--------+------+------+---------------+
      16|     1|    12|               |
      17|    18|    38|               |
      18|    17|    65|               |
      19|     5|   157|               |
      20|     0|    75|               |
      21|     4|    87|               |
      22|    15|    50|               |
      23|   864|  1364|               |
      24|     0|   476|               |
      25|    23|   257|               |
=======================================================================================================================
===========
CopyOptions
===========

Copy Command Syntax
COPY INTO
	<table_Name>
FROM
	Internal/externalStage
FILES = ('<file_ name>',,'<file_ name2>')
FILE_FORMAT = <file_format_name>
copyOptions

CopyOptions
===========

a. ON_ERROR = CONTINUE
b. VALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS (Validate the data files instead of loading them)

RETURN_n_ROWS (e. RETURN_10_ROWS) 
=============
	Validates & returns the specified number of rows;
	fails at the first error encountered
RETURN_ERRORS
=============
	Returns all errors in Copy Command

c. SIZE_LIMIT = num
Specify maximum size (in bytes) of data loaded in that command (at least one file)
When the threshold is exceeded, the COPY operation stops loading
Threshold for each file
DEFAULT: null (no size limit)

d. RETURN_FAILED_ONLY = TRUE | FALSE
Specifies whether to return only files that have failed to load in the statement result
DEFAULT = FALSE

e. TRUNCATECOLUMNS = TRUE | FALSE
Specifies whether to truncate text strings that exceed the target column length
TRUE = strings are automatically truncated to the target column length
FALSE = COPY produces an error if a loaded string exceeds the target column length
DEFAULT = FALSE

f. FORCE = TRUE | FALSE
Specifies to load all files, regardless of whether theyâ€™ve been loaded previously and have not changed since they were loaded 
Note: that this option reloads files, potentially duplicating data in a table.

g. PURGE = TRUE | FALSE
specifies whether to remove the data files from the stage automatically after the data is loaded successfully
DEFAULT: FALSE

// Create new stage
CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage_errorex
url='s3://bucketsnowflakes4'

// List files in stage
LIST @MANAGE_DB.external_stages.aws_stage_errorex;

name                                          |size |md5                             |last_modified               |
----------------------------------------------+-----+--------------------------------+----------------------------+
s3://bucketsnowflakes4/OrderDetails_error.csv |54622|99bb5d5b87e74256ca04c91359204dba|Wed, 7 Apr 2021 09:26:50 GMT|
s3://bucketsnowflakes4/OrderDetails_error2.csv|10512|02dd466971414c7394772d108397374a|Wed, 7 Apr 2021 09:58:53 GMT|

These files contains some invalid data

// Create example table
CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX (
ORDER_ID VARCHAR(30),
AMOUNT INT,
PROFIT INT,
QUANTITY INT,
CATEGORY VARCHAR(30),
SUBCATEGORY VARCHAR(30));

//Demonstrating error message, the below copy command will fail due to invalid data`
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
FROM @MANAGE_DB.external_stages.aws_stage_errorex
file_format= (type = csv field_delimiter=',' skip_header=1)
files = ('OrderDetails_error.csv');

SQL Error [100038] [22018]: Numeric value 'one thousand' is not recognized
  File 'OrderDetails_error.csv', line 2, character 14
  Row 1, column "ORDERS_EX"["PROFIT":3]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. 
  For more information on loading options, please run 'info loading_data' in a SQL client


// Validating table is empty    
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;    


// Error handling using the ON_ERROR option
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
FROM @MANAGE_DB.external_stages.aws_stage_errorex
file_format= (type = csv field_delimiter=',' skip_header=1)
files = ('OrderDetails_error.csv')
ON_ERROR = 'CONTINUE';

Updated Rows	1498

// Validating results and truncating table 
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX limit 10;
ORDER_ID|AMOUNT|PROFIT|QUANTITY|CATEGORY   |SUBCATEGORY     |
--------+------+------+--------+-----------+----------------+
B-25601 |     8|    -2|       3|Clothing   |Hankerchief     |
B-25601 |    80|   -56|       4|Electronics|Electronic Games|
B-25602 |   168|  -111|       2|Electronics|Phones          |
B-25602 |   424|  -272|       5|Electronics|Phones          |
B-25602 |  2617|  1151|       4|Electronics|Phones          |
B-25602 |   561|   212|       3|Clothing   |Saree           |
B-25602 |   119|    -5|       8|Clothing   |Saree           |
B-25603 |  1355|   -60|       5|Clothing   |Trousers        |
B-25603 |    24|   -30|       1|Furniture  |Chairs          |
B-25603 |   193|  -166|       3|Clothing   |Saree           |

SELECT COUNT(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
    1498|

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;

// Error handling using the ON_ERROR option = ABORT_STATEMENT (default), the statement will get aborted even when ON_ERROR is skiped in the copy command.
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = 'ABORT_STATEMENT';
	
SQL Error [100038] [22018]: Numeric value 'one thousand' is not recognized
  File 'OrderDetails_error.csv', line 2, character 14
  Row 1, column "ORDERS_EX"["PROFIT":3]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. 
  For more information on loading options, please run 'info loading_data' in a SQL client.


  // Validating results and truncating table 
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
SELECT COUNT(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
       0|

SKIP_FILE will ignore the file which has invalid data if any.

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;

// Error handling using the ON_ERROR option = SKIP_FILE
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = 'SKIP_FILE';
	
file											status		rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes4/OrderDetails_error2.csv	LOADED		285			285			1			0				
s3://bucketsnowflakes4/OrderDetails_error.csv	LOAD_FAILED	1500		0			1			2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS_EX"["PROFIT":3]
  
// Validating results and truncating table 
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
SELECT COUNT(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
     285|

Continue Vs SKIP_FILE
Continue will ignore the invalid records and proceed with remaining rows.
Skip_file will ignore the entire file when a invalid record found.

SKIP_FILE_<number>
==================
This options allow us to define the maximum allowed invalid records for a file, records will get ignored and will get loaded until it reaches the allowed limit, 
once the error record count reached the maximum error specified, then it will ignore the entire file.

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;    

// Error handling using the ON_ERROR option = SKIP_FILE_<number>
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = 'SKIP_FILE_2';
    
file											status		rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes4/OrderDetails_error2.csv	LOADED		285			285			2			0				
s3://bucketsnowflakes4/OrderDetails_error.csv	LOAD_FAILED	1500		0			2			2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS_EX"["PROFIT":3]

in the above example the maximum error limit is 2 once the limit is reached, system will ignore the file.
    
// Validating results and truncating table 
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
SELECT COUNT(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
     285|

retry the same with error limit as 3.

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;    

// Error handling using the ON_ERROR option = SKIP_FILE_<number>
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = 'SKIP_FILE_3';
    
file											status				rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes4/OrderDetails_error2.csv	LOADED				285			285			3			0				
s3://bucketsnowflakes4/OrderDetails_error.csv	PARTIALLY_LOADED	1500		1498		3			2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS_EX"["PROFIT":3]

in the above example the maximum error limit is 3 but the no of errors in the file is 2 so, it ignored the errored records and processed the remaining records 
in the file.
    
// Validating results and truncating table 
SELECT * FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
SELECT COUNT(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
    1783|

SKIP_FILE_N%
This option will allow us to define the allowable error records in percentage, the functionality is same as skip_file_number, only difference is it will dynamically
find the number of records can be igmored for each file on the fly based on the total number of records by applying the % on the total record count.

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;    
    
// Error handling using the ON_ERROR option = SKIP_FILE_<number>
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = 'SKIP_FILE_0.5%'; 
  
file											status				rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes4/OrderDetails_error2.csv	LOADED				285			285	1		0				
s3://bucketsnowflakes4/OrderDetails_error.csv	PARTIALLY_LOADED	1500		1498		7			2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS_EX"["PROFIT":3]

  
SELECT count(*) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(*)|
--------+
    1783|

CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX (
    ORDER_ID VARCHAR(30),
    AMOUNT INT,
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));
	
SIZE_LIMIT
==========

SIZE_LIMIT (> 0) that specifies the maximum size (in bytes) of data to be loaded for a given COPY statement. When the threshold is exceeded, the COPY operation 
discontinues loading files. This option is commonly used to load a common group of files using multiple COPY statements. For each statement, the data load 
continues until the specified SIZE_LIMIT is exceeded, before moving on to the next statement.

For example, suppose a set of files in a stage path were each 10 MB in size. If multiple COPY statements set SIZE_LIMIT to 25000000 (25 MB), each would 
load 3 files. That is, each COPY operation would discontinue after the SIZE_LIMIT threshold was exceeded.

Note that at least one file is loaded regardless of the value specified for SIZE_LIMIT unless there is no file to be loaded.

LIST @MANAGE_DB.external_stages.aws_stage_errorex;
name											size	md5									last_modified
s3://bucketsnowflakes4/OrderDetails_error.csv	54622	99bb5d5b87e74256ca04c91359204dba	Wed, 7 Apr 2021 09:26:50 GMT
s3://bucketsnowflakes4/OrderDetails_error2.csv	10512	02dd466971414c7394772d108397374a	Wed, 7 Apr 2021 09:58:53 GMT

TRUNCATE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX;  

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (type = csv field_delimiter=',' skip_header=1)
    files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
    ON_ERROR = SKIP_FILE_3 
    SIZE_LIMIT = 30;

file											status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://bucketsnowflakes4/OrderDetails_error2.csv	LOADED	285			285			3			0				

File Format Object
==================

// Creating schema to keep things organized
CREATE OR REPLACE SCHEMA MANAGE_DB.file_formats;

// Creating file format object
CREATE OR REPLACE file format MANAGE_DB.file_formats.my_file_format;

// See properties of file format object
DESC file format MANAGE_DB.file_formats.my_file_format;

property                      |property_type|property_value|property_default|
------------------------------+-------------+--------------+----------------+
TYPE                          |String       |CSV           |CSV             |
RECORD_DELIMITER              |String       |\n            |\n              |
FIELD_DELIMITER               |String       |,             |,               |
FILE_EXTENSION                |String       |              |                |
SKIP_HEADER                   |Integer      |0             |0               |
PARSE_HEADER                  |Boolean      |false         |false           |
DATE_FORMAT                   |String       |AUTO          |AUTO            |
TIME_FORMAT                   |String       |AUTO          |AUTO            |
TIMESTAMP_FORMAT              |String       |AUTO          |AUTO            |
BINARY_FORMAT                 |String       |HEX           |HEX             |
ESCAPE                        |String       |NONE          |NONE            |
ESCAPE_UNENCLOSED_FIELD       |String       |\\            |\\              |
TRIM_SPACE                    |Boolean      |false         |false           |
FIELD_OPTIONALLY_ENCLOSED_BY  |String       |NONE          |NONE            |
NULL_IF                       |List         |[\\N]         |[\\N]           |
COMPRESSION                   |String       |AUTO          |AUTO            |
ERROR_ON_COLUMN_COUNT_MISMATCH|Boolean      |true          |true            |
VALIDATE_UTF8                 |Boolean      |true          |true            |
SKIP_BLANK_LINES              |Boolean      |false         |false           |
REPLACE_INVALID_CHARACTERS    |Boolean      |false         |false           |
EMPTY_FIELD_AS_NULL           |Boolean      |true          |true            |
SKIP_BYTE_ORDER_MARK          |Boolean      |true          |true            |
ENCODING                      |String       |UTF8          |UTF8            |

Review the above properties carefully, while creating the file format nothing was specified except the file format name, hence all the properties are 
populated with default values. 

So, the default file format type is CSV, default field delimiter is ',' and skip_hear is 0.

// Using file format object in Copy command, this will fail to load data as skip header is 0(no header info in the file) it consider the 1 line as data but line 1 contains 
column header and the entire row is string.
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (FORMAT_NAME=MANAGE_DB.file_formats.my_file_format)
    files = ('OrderDetails_error.csv')
    ON_ERROR = 'SKIP_FILE_3'; 

select count(1) from OUR_FIRST_DB.PUBLIC.ORDERS_EX;

COUNT(1)|
--------+
       0|

// Altering file format object
ALTER file format MANAGE_DB.file_formats.my_file_format
    SET SKIP_HEADER = 1;
	
// See properties of file format object
DESC file format MANAGE_DB.file_formats.my_file_format;
property                      |property_type|property_value|property_default|
------------------------------+-------------+--------------+----------------+
TYPE                          |String       |CSV           |CSV             |
RECORD_DELIMITER              |String       |\n            |\n              |
FIELD_DELIMITER               |String       |,             |,               |
FILE_EXTENSION                |String       |              |                |
SKIP_HEADER                   |Integer      |1             |0               |
PARSE_HEADER                  |Boolean      |false         |false           |
DATE_FORMAT                   |String       |AUTO          |AUTO            |
TIME_FORMAT                   |String       |AUTO          |AUTO            |
TIMESTAMP_FORMAT              |String       |AUTO          |AUTO            |
BINARY_FORMAT                 |String       |HEX           |HEX             |
ESCAPE                        |String       |NONE          |NONE            |
ESCAPE_UNENCLOSED_FIELD       |String       |\\            |\\              |
TRIM_SPACE                    |Boolean      |false         |false           |
FIELD_OPTIONALLY_ENCLOSED_BY  |String       |NONE          |NONE            |
NULL_IF                       |List         |[\\N]         |[\\N]           |
COMPRESSION                   |String       |AUTO          |AUTO            |
ERROR_ON_COLUMN_COUNT_MISMATCH|Boolean      |true          |true            |
VALIDATE_UTF8                 |Boolean      |true          |true            |
SKIP_BLANK_LINES              |Boolean      |false         |false           |
REPLACE_INVALID_CHARACTERS    |Boolean      |false         |false           |
EMPTY_FIELD_AS_NULL           |Boolean      |true          |true            |
SKIP_BYTE_ORDER_MARK          |Boolean      |true          |true            |
ENCODING                      |String       |UTF8          |UTF8            |


COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (FORMAT_NAME=MANAGE_DB.file_formats.my_file_format)
    files = ('OrderDetails_error.csv')
    ON_ERROR = 'SKIP_FILE_3'; 

COUNT(1)|
--------+
    1498|

// Defining properties on creation of file format object   
CREATE OR REPLACE file format MANAGE_DB.file_formats.my_file_format
    TYPE=JSON,
    TIME_FORMAT=AUTO;    
    
// See properties of file format object    
DESC file format MANAGE_DB.file_formats.my_file_format;   

property                  |property_type|property_value|property_default|
--------------------------+-------------+--------------+----------------+
TYPE                      |String       |JSON          |CSV             |
FILE_EXTENSION            |String       |              |                |
DATE_FORMAT               |String       |AUTO          |AUTO            |
TIME_FORMAT               |String       |AUTO          |AUTO            |
TIMESTAMP_FORMAT          |String       |AUTO          |AUTO            |
BINARY_FORMAT             |String       |HEX           |HEX             |
TRIM_SPACE                |Boolean      |false         |false           |
NULL_IF                   |List         |[]            |[\\N]           |
COMPRESSION               |String       |AUTO          |AUTO            |
ENABLE_OCTAL              |Boolean      |false         |false           |
ALLOW_DUPLICATE           |Boolean      |false         |false           |
STRIP_OUTER_ARRAY         |Boolean      |false         |false           |
STRIP_NULL_VALUES         |Boolean      |false         |false           |
IGNORE_UTF8_ERRORS        |Boolean      |false         |false           |
REPLACE_INVALID_CHARACTERS|Boolean      |false         |false           |
SKIP_BYTE_ORDER_MARK      |Boolean      |true          |true            |

truncate table OUR_FIRST_DB.PUBLIC.ORDERS_EX;
  
// Using file format object in Copy command , this will fail to load data as per the file format it expect the data in json format, but the files are in CSV format.     
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM @MANAGE_DB.external_stages.aws_stage_errorex
    file_format= (FORMAT_NAME=MANAGE_DB.file_formats.my_file_format)
    files = ('OrderDetails_error.csv')
    ON_ERROR = 'SKIP_FILE_3'; 
	
SQL Error [2019] [0A000]: SQL compilation error:
JSON file format can produce one and only one column of type variant, object, or array. Load data into separate columns using the MATCH_BY_COLUMN_NAME 
copy option or copy with transformation.


// Altering the type of a file format is not possible, the below will fail
ALTER file format MANAGE_DB.file_formats.my_file_format
SET TYPE = CSV;

SQL Error [2134] [42601]: SQL compilation error:
File format type cannot be changed.


// Recreate file format (default = CSV)
CREATE OR REPLACE file format MANAGE_DB.file_formats.my_file_format


// See properties of file format object    
DESC file format MANAGE_DB.file_formats.my_file_format;   

property                      |property_type|property_value|property_default|
------------------------------+-------------+--------------+----------------+
TYPE                          |String       |CSV           |CSV             |
RECORD_DELIMITER              |String       |\n            |\n              |
FIELD_DELIMITER               |String       |,             |,               |
FILE_EXTENSION                |String       |              |                |
SKIP_HEADER                   |Integer      |0             |0               |
PARSE_HEADER                  |Boolean      |false         |false           |
DATE_FORMAT                   |String       |AUTO          |AUTO            |
TIME_FORMAT                   |String       |AUTO          |AUTO            |
TIMESTAMP_FORMAT              |String       |AUTO          |AUTO            |
BINARY_FORMAT                 |String       |HEX           |HEX             |
ESCAPE                        |String       |NONE          |NONE            |
ESCAPE_UNENCLOSED_FIELD       |String       |\\            |\\              |
TRIM_SPACE                    |Boolean      |false         |false           |
FIELD_OPTIONALLY_ENCLOSED_BY  |String       |NONE          |NONE            |
NULL_IF                       |List         |[\\N]         |[\\N]           |
COMPRESSION                   |String       |AUTO          |AUTO            |
ERROR_ON_COLUMN_COUNT_MISMATCH|Boolean      |true          |true            |
VALIDATE_UTF8                 |Boolean      |true          |true            |
SKIP_BLANK_LINES              |Boolean      |false         |false           |
REPLACE_INVALID_CHARACTERS    |Boolean      |false         |false           |
EMPTY_FIELD_AS_NULL           |Boolean      |true          |true            |
SKIP_BYTE_ORDER_MARK          |Boolean      |true          |true            |
ENCODING                      |String       |UTF8          |UTF8            |

// Truncate table
TRUNCATE table OUR_FIRST_DB.PUBLIC.ORDERS_EX;

we can override the property values in the copy into statement. As per the above properties skip header is 0, however if we want to specify it as 1 while loading 
it is possible. Check the below copy into command file_format option, compare it with the previous copy into statements.


// Overwriting properties of file format object      
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
    FROM  @MANAGE_DB.external_stages.aws_stage_errorex
    file_format = (FORMAT_NAME= MANAGE_DB.file_formats.my_file_format  field_delimiter = ',' skip_header=1 )
    files = ('OrderDetails_error.csv')
    ON_ERROR = 'SKIP_FILE_3'; 
	
SELECT count(1) FROM OUR_FIRST_DB.PUBLIC.ORDERS_EX;
COUNT(1)|
--------+
    1498|

//describe the stage object to see it's parameters
DESC STAGE MANAGE_DB.external_stages.aws_stage_errorex;

parent_property   |property                      |property_type|property_value            |property_default|
------------------+------------------------------+-------------+--------------------------+----------------+
STAGE_FILE_FORMAT |TYPE                          |String       |CSV                       |CSV             |
STAGE_FILE_FORMAT |RECORD_DELIMITER              |String       |\n                        |\n              |
STAGE_FILE_FORMAT |FIELD_DELIMITER               |String       |,                         |,               |
STAGE_FILE_FORMAT |FILE_EXTENSION                |String       |                          |                |
STAGE_FILE_FORMAT |SKIP_HEADER                   |Integer      |0                         |0               |
STAGE_FILE_FORMAT |PARSE_HEADER                  |Boolean      |false                     |false           |
STAGE_FILE_FORMAT |DATE_FORMAT                   |String       |AUTO                      |AUTO            |
STAGE_FILE_FORMAT |TIME_FORMAT                   |String       |AUTO                      |AUTO            |
STAGE_FILE_FORMAT |TIMESTAMP_FORMAT              |String       |AUTO                      |AUTO            |
STAGE_FILE_FORMAT |BINARY_FORMAT                 |String       |HEX                       |HEX             |
STAGE_FILE_FORMAT |ESCAPE                        |String       |NONE                      |NONE            |
STAGE_FILE_FORMAT |ESCAPE_UNENCLOSED_FIELD       |String       |\\                        |\\              |
STAGE_FILE_FORMAT |TRIM_SPACE                    |Boolean      |false                     |false           |
STAGE_FILE_FORMAT |FIELD_OPTIONALLY_ENCLOSED_BY  |String       |NONE                      |NONE            |
STAGE_FILE_FORMAT |NULL_IF                       |List         |[\\N]                     |[\\N]           |
STAGE_FILE_FORMAT |COMPRESSION                   |String       |AUTO                      |AUTO            |
STAGE_FILE_FORMAT |ERROR_ON_COLUMN_COUNT_MISMATCH|Boolean      |true                      |true            |
STAGE_FILE_FORMAT |VALIDATE_UTF8                 |Boolean      |true                      |true            |
STAGE_FILE_FORMAT |SKIP_BLANK_LINES              |Boolean      |false                     |false           |
STAGE_FILE_FORMAT |REPLACE_INVALID_CHARACTERS    |Boolean      |false                     |false           |
STAGE_FILE_FORMAT |EMPTY_FIELD_AS_NULL           |Boolean      |true                      |true            |
STAGE_FILE_FORMAT |SKIP_BYTE_ORDER_MARK          |Boolean      |true                      |true            |
STAGE_FILE_FORMAT |ENCODING                      |String       |UTF8                      |UTF8            |
STAGE_COPY_OPTIONS|ON_ERROR                      |String       |ABORT_STATEMENT           |ABORT_STATEMENT |
STAGE_COPY_OPTIONS|SIZE_LIMIT                    |Long         |                          |                |
STAGE_COPY_OPTIONS|PURGE                         |Boolean      |false                     |false           |
STAGE_COPY_OPTIONS|RETURN_FAILED_ONLY            |Boolean      |false                     |false           |
STAGE_COPY_OPTIONS|ENFORCE_LENGTH                |Boolean      |true                      |true            |
STAGE_COPY_OPTIONS|TRUNCATECOLUMNS               |Boolean      |false                     |false           |
STAGE_COPY_OPTIONS|FORCE                         |Boolean      |false                     |false           |
STAGE_LOCATION    |URL                           |String       |["s3://bucketsnowflakes4"]|                |
STAGE_CREDENTIALS |AWS_KEY_ID                    |String       |                          |                |
DIRECTORY         |ENABLE                        |Boolean      |false                     |false           |
DIRECTORY         |AUTO_REFRESH                  |Boolean      |false                     |false           |

if we don't specify any file formats then the default file formats from the stage will be used.


Assignment 3
============
If you have not created the database EXERCISE_DB then you can do so.
The same goes for the customer table with the following columns:
ID INT,
first_name varchar,
last_name varchar,
email varchar,
age int,
city varchar


create table if not exists customer(
ID INT,
first_name varchar,
last_name varchar,
email varchar,
age int,
city varchar)


1. Create a database called EXERCISE_DB (if you have created that in one of the previous lectures you can skip this step)

use role sysadmin;

create database if not exists exercise_db;

use exercise_db;

create table if not exists customer(
ID INT,
first_name varchar,
last_name varchar,
email varchar,
age int,
city varchar);


2. Create a stage object

The data is available under: s3://snowflake-assignments-mc/loadingdata/

Data type: CSV - delimited by ';' (semicolon)

Header is in the first line.

CREATE OR REPLACE STAGE exercise_db.public.aws_stage
    url='s3://snowflake-assignments-mc/loadingdata/';
	
create file format if not exists exercise_db.public.my_csv_file_format_1
type = csv  
field_delimiter = ';'
skip_header = 1;

describe file_format exercise_db.public.my_csv_file_format_1;
property                      |property_type|property_value|property_default|
------------------------------+-------------+--------------+----------------+
TYPE                          |String       |csv           |CSV             |
RECORD_DELIMITER              |String       |\n            |\n              |
FIELD_DELIMITER               |String       |,             |,               |
FILE_EXTENSION                |String       |              |                |
SKIP_HEADER                   |Integer      |1             |0               |
PARSE_HEADER                  |Boolean      |false         |false           |
DATE_FORMAT                   |String       |AUTO          |AUTO            |
TIME_FORMAT                   |String       |AUTO          |AUTO            |
TIMESTAMP_FORMAT              |String       |AUTO          |AUTO            |
BINARY_FORMAT                 |String       |HEX           |HEX             |
ESCAPE                        |String       |NONE          |NONE            |
ESCAPE_UNENCLOSED_FIELD       |String       |\\            |\\              |
TRIM_SPACE                    |Boolean      |false         |false           |
FIELD_OPTIONALLY_ENCLOSED_BY  |String       |NONE          |NONE            |
NULL_IF                       |List         |[\\N]         |[\\N]           |
COMPRESSION                   |String       |AUTO          |AUTO            |
ERROR_ON_COLUMN_COUNT_MISMATCH|Boolean      |true          |true            |
VALIDATE_UTF8                 |Boolean      |true          |true            |
SKIP_BLANK_LINES              |Boolean      |false         |false           |
REPLACE_INVALID_CHARACTERS    |Boolean      |false         |false           |
EMPTY_FIELD_AS_NULL           |Boolean      |true          |true            |
SKIP_BYTE_ORDER_MARK          |Boolean      |true          |true            |
ENCODING                      |String       |UTF8          |UTF8            |

3. List the files in the table

list @exercise_db.public.aws_stage;

name                                                    |size |md5                             |last_modified               |
--------------------------------------------------------+-----+--------------------------------+----------------------------+
s3://snowflake-assignments-mc/loadingdata/customers2.csv|40873|c0e17482e2f37a8dc73fdc22f8b06ec0|Mon, 3 May 2021 16:03:29 GMT|
s3://snowflake-assignments-mc/loadingdata/customers3.csv|46319|f2480711a8924658916f4668eb8a94d7|Mon, 3 May 2021 16:03:37 GMT|


4. Load the data in the existing customers table using the COPY command

select count(1) from exercise_db.PUBLIC.customer;
COUNT(1)|
--------+
       0|

COPY INTO exercise_db.PUBLIC.customer
    FROM  @exercise_db.public.aws_stage
    file_format = (FORMAT_NAME= exercise_db.public.my_csv_file_format_1);  

Questions for this assignment
How many rows have been loaded in this assignment?

select count(1) from exercise_db.PUBLIC.customer;
COUNT(1)|
--------+
    1600|
	
Quiz
====
Q1. We want to load data from a file on an S3 stage. In this file we encounter a few errors. We want to load all rows that can be loaded regardless. 
What value can we set the ON_ERROR option to?
Answer : CONTINUE
Q2. What is a file format object created for?
To store information about file properties like type, skip_header, field_delimiter etc.


Assignment 4
If you have not created the database EXERCISE_DB then you can do so. The same goes for the customer table with the following columns:
ID INT,
first_name varchar,
last_name varchar,
email varchar,
age int,
city varchar

1. Create a stage & file format object

The data is available under: s3://snowflake-assignments-mc/fileformat/

Data type: CSV - delimited by '|' (pipe)

Header is in the first line.

use role sysadmin;
use exercise_db;

CREATE OR REPLACE STAGE exercise_db.public.aws_stage_1
    url='s3://snowflake-assignments-mc/fileformat';
	
create file format if not exists exercise_db.public.my_csv_file_format_2
type = csv  
field_delimiter = '|'
skip_header = 1;

2. List the files in the table

list @exercise_db.public.aws_stage_1;

name                                                   |size |md5                             |last_modified               |
-------------------------------------------------------+-----+--------------------------------+----------------------------+
s3://snowflake-assignments-mc/fileformat/customers4.csv|13512|9de6f28c9e8428650f3972f90018a7ec|Mon, 3 May 2021 16:15:57 GMT|

3. Load the data in the existing customers table using the COPY command your stage and the created file format object.

COPY INTO exercise_db.PUBLIC.customer
    FROM  @exercise_db.public.aws_stage_1
    file_format = (FORMAT_NAME= exercise_db.public.my_csv_file_format_2); 
	
Updated Rows	250

4. How many rows have been loaded in this assignment?
250

================================================================================================================================
End of Topic - Loading Data
================================================================================================================================

copy options in detail
======================

VALIDATION_MODE
===============

VALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS
Validate the data files instead of loading them

RETURN_n_ROWS(e. RETURN_10_ROWS)
	Validates & returns the specified number of rows;
	fails at the first error encountered	
RETURN_ERRORS
	Returns all errors in Copy Command
	
use role sysadmin;

// Prepare database & table
CREATE OR REPLACE DATABASE COPY_DB;


CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
  
LIST @COPY_DB.PUBLIC.aws_stage_copy;

name												size	md5									last_modified
s3://snowflakebucket-copyoption/size/Orders.csv		54600	1a1c4a47a8e8e43ecef5bf8a46ee4017	Wed, 28 Apr 2021 16:14:06 GMT
s3://snowflakebucket-copyoption/size/Orders2.csv	54598	36bcccace29563ceb1f86a62f599dba3	Wed, 28 Apr 2021 16:29:32 GMT
    
 //Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS;
	
ERROR|FILE|LINE|CHARACTER|BYTE_OFFSET|CATEGORY|CODE|SQL_STATE|COLUMN_NAME|ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD|
-----+----+----+---------+-----------+--------+----+---------+-----------+----------+--------------+---------------+

This means there is no invalid data in the input files.

select * from COPY_DB.PUBLIC.ORDERS;
no records to display as copy into will not populate data into the table when validation_mode is set. Instead it will validate the data in the file.
    
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
   VALIDATION_MODE = RETURN_5_ROWS;

ORDER_ID|AMOUNT|PROFIT|QUANTITY|CATEGORY   |SUBCATEGORY     |
--------+------+------+--------+-----------+----------------+
B-25601 |12745 |    23|       7|Furniture  |Bookcases       |
B-25601 |66    |   223|       5|Clothing   |Stole           |
B-25601 |8     |    -2|       3|Clothing   |Hankerchief     |
B-25601 |80    |   -56|       4|Electronics|Electronic Games|
B-25602 |168   |  -111|       2|Electronics|Phones          |

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
	
SELECT count(1) FROM COPY_DB.PUBLIC.ORDERS;
COUNT(1)|
--------+
    3000|

--Scenario with error records
   
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/returnfailed/';


LIST @COPY_DB.PUBLIC.aws_stage_copy;

name                                                                       |size |md5                             |last_modified                |
---------------------------------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error.csv        |54622|99bb5d5b87e74256ca04c91359204dba|Wed, 28 Apr 2021 16:36:24 GMT|
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error2 - Copy.csv|10514|7c9dc0c0c8e6a9b82173c3df091a746a|Thu, 29 Apr 2021 08:02:50 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders.csv                    |54597|cf56c5cfede468cdcacac48d7ab67e75|Wed, 28 Apr 2021 16:36:06 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders2.csv                   |54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:36:07 GMT|

truncate table COPY_DB.PUBLIC.ORDERS;
  
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS;
	
ERROR                                               |FILE                                       |LINE|CHARACTER|BYTE_OFFSET|CATEGORY  |CODE  |SQL_STATE|COLUMN_NAME           |ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD                                 |
----------------------------------------------------+-------------------------------------------+----+---------+-----------+----------+------+---------+----------------------+----------+--------------+------------------------------------------------+
Numeric value '7-' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   2|       17|         71|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         1|             2|B-30601,1275,10,7-,Furniture,BookcasesÂ¶         |
Numeric value '3a' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   4|       16|        143|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         3|             4|B-30601,8,-244,3a,Clothing,HankerchiefÂ¶         |
Numeric value 'one thousand' is not recognized      |returnfailed/OrderDetails_error.csv        |   2|       14|         68|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         1|             2|B-25601,1275,one thousand,7,Furniture,BookcasesÂ¶|
Numeric value 'two hundred twenty' is not recognized|returnfailed/OrderDetails_error.csv        |   3|       12|        115|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         2|             3|B-25601,66,two hundred twenty,5,Clothing,StoleÂ¶ |

select * from COPY_DB.PUBLIC.ORDERS;
no data to display
    
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
   VALIDATION_MODE = RETURN_5_ROWS;
   
this will return a error.
SQL Error [100038] [22018]: Numeric value '7-' is not recognized
  File 'returnfailed/OrderDetails_error2 - Copy.csv', line 2, character 17
  Row 1, column "ORDERS"["QUANTITY":4]

========Assignment done by me
1. Create employees table in the copy_db if not exists with the following columns
customer_id int,
first_name varchar(50),
last_name varchar(50),
email varchar(50),
age int,
department varchar(50)
  
create table COPY_DB.PUBLIC.employees(
 customer_id int,
  first_name varchar(50),
  last_name varchar(50),
  email varchar(50),
  age int,
  department varchar(50)
);

2. create a stage object for the given URL s3://snowflake-assignments-mc/copyoptions/example1
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
url='s3://snowflake-assignments-mc/copyoptions/example1';

3. list the files in the stage object created in the previous step
list @COPY_DB.PUBLIC.aws_stage_copy

name                                                            |size|md5                             |last_modified               |
----------------------------------------------------------------+----+--------------------------------+----------------------------+
s3://snowflake-assignments-mc/copyoptions/example1/employees.csv|6965|9f89433257464990e7cd95f515555808|Mon, 3 May 2021 17:09:22 GMT|

4. validate the data in the input files and return the errored records in the input files if any.

truncate table MANAGE_DB.PUBLIC.CUSTOMERS;

COPY INTO copy_DB.PUBLIC.employees
    FROM @COPY_DB.PUBLIC.aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*emp.*'
    VALIDATION_MODE = RETURN_ERRORS;
	
ERROR                              |FILE                              |LINE|CHARACTER|BYTE_OFFSET|CATEGORY  |CODE  |SQL_STATE|COLUMN_NAME                 |ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD                                            |
-----------------------------------+----------------------------------+----+---------+-----------+----------+------+---------+----------------------------+----------+--------------+-----------------------------------------------------------+
Numeric value '-' is not recognized|copyoptions/example1/employees.csv|  10|        1|        463|conversion|100038|22018    |"EMPLOYEES"["CUSTOMER_ID":1]|         9|            10|-,Sutherland,Deinhard,sdeinhard8@wunderground.com,81,LegalÂ¶|

ignore upto 2 errored records in each file and load the data from each file. if more than 2 errors exists skip the file.

COPY INTO copy_DB.PUBLIC.employees
    FROM @COPY_DB.PUBLIC.aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*emp.*'
    ON_ERROR = 'SKIP_FILE_3';
	
or	

COPY INTO copy_DB.PUBLIC.employees
    FROM @COPY_DB.PUBLIC.aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*emp.*'
    ON_ERROR = 'CONTINUE';   ---As there is only one error in the input file this should be more than enough in this scenario.
	
5. find the number of records populated into the table.
select count(1) from copy_DB.PUBLIC.employees;
COUNT(1)|
--------+
     121|
	 
Working with error records
==========================

CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/returnfailed/';

LIST @COPY_DB.PUBLIC.aws_stage_copy;    
name                                                                       |size |md5                             |last_modified                |
---------------------------------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error.csv        |54622|99bb5d5b87e74256ca04c91359204dba|Wed, 28 Apr 2021 16:36:24 GMT|
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error2 - Copy.csv|10514|7c9dc0c0c8e6a9b82173c3df091a746a|Thu, 29 Apr 2021 08:02:50 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders.csv                    |54597|cf56c5cfede468cdcacac48d7ab67e75|Wed, 28 Apr 2021 16:36:06 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders2.csv                   |54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:36:07 GMT|

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    VALIDATION_MODE = RETURN_ERRORS;

ERROR                                               |FILE                                       |LINE|CHARACTER|BYTE_OFFSET|CATEGORY  |CODE  |SQL_STATE|COLUMN_NAME           |ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD                                 |
----------------------------------------------------+-------------------------------------------+----+---------+-----------+----------+------+---------+----------------------+----------+--------------+------------------------------------------------+
Numeric value '7-' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   2|       17|         71|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         1|             2|B-30601,1275,10,7-,Furniture,BookcasesÂ¶         |
Numeric value '3a' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   4|       16|        143|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         3|             4|B-30601,8,-244,3a,Clothing,HankerchiefÂ¶         |
Numeric value 'one thousand' is not recognized      |returnfailed/OrderDetails_error.csv        |   2|       14|         68|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         1|             2|B-25601,1275,one thousand,7,Furniture,BookcasesÂ¶|
Numeric value 'two hundred twenty' is not recognized|returnfailed/OrderDetails_error.csv        |   3|       12|        115|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         2|             3|B-25601,66,two hundred twenty,5,Clothing,StoleÂ¶ |

create table rejected_order_records as
select current_date() as run_date, current_timestamp() as run_ts, t.* from table(result_scan(last_query_id())) as t;

select *
from rejected_order_records;

RUN_DATE  |RUN_TS                 |ERROR                                               |FILE                                       |LINE|CHARACTER|BYTE_OFFSET|CATEGORY  |CODE  |SQL_STATE|COLUMN_NAME           |ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD                                 |
----------+-----------------------+----------------------------------------------------+-------------------------------------------+----+---------+-----------+----------+------+---------+----------------------+----------+--------------+------------------------------------------------+
2024-06-25|2024-06-26 01:16:53.204|Numeric value 'one thousand' is not recognized      |returnfailed/OrderDetails_error.csv        |   2|       14|         68|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         1|             2|B-25601,1275,one thousand,7,Furniture,BookcasesÂ¶|
2024-06-25|2024-06-26 01:16:53.204|Numeric value 'two hundred twenty' is not recognized|returnfailed/OrderDetails_error.csv        |   3|       12|        115|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         2|             3|B-25601,66,two hundred twenty,5,Clothing,StoleÂ¶ |
2024-06-25|2024-06-26 01:16:53.204|Numeric value '7-' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   2|       17|         71|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         1|             2|B-30601,1275,10,7-,Furniture,BookcasesÂ¶         |
2024-06-25|2024-06-26 01:16:53.204|Numeric value '3a' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   4|       16|        143|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         3|             4|B-30601,8,-244,3a,Clothing,HankerchiefÂ¶         |


SELECT 
SPLIT_PART(rejected_record,',',1) as ORDER_ID, 
SPLIT_PART(rejected_record,',',2) as AMOUNT, 
SPLIT_PART(rejected_record,',',3) as PROFIT, 
SPLIT_PART(rejected_record,',',4) as QUATNTITY, 
SPLIT_PART(rejected_record,',',5) as CATEGORY, 
SPLIT_PART(rejected_record,',',6) as SUBCATEGORY
FROM rejected_order_records;

ORDER_ID|AMOUNT|PROFIT            |QUATNTITY|CATEGORY |SUBCATEGORY |
--------+------+------------------+---------+---------+------------+
B-25601 |1275  |one thousand      |7        |Furniture|BookcasesÂ¶  |
B-25601 |66    |two hundred twenty|5        |Clothing |StoleÂ¶      |
B-30601 |1275  |10                |7-       |Furniture|BookcasesÂ¶  |
B-30601 |8     |-244              |3a       |Clothing |HankerchiefÂ¶|

another approach to handle error records without using validation_mode

truncate table COPY_DB.PUBLIC.ORDERS;
truncate table COPY_DB.PUBLIC.rejected_order_records;

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    ON_ERROR=CONTINUE;

file																		status				rows_parsed		rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error2 - Copy.csv	PARTIALLY_LOADED	285				283			285			2			Numeric value '7-' is not recognized	2	17	"ORDERS"["QUANTITY":4]
s3://snowflakebucket-copyoption/returnfailed/Orders2.csv					LOADED				1500			1500		1500		0				
s3://snowflakebucket-copyoption/returnfailed/Orders.csv						LOADED				1500			1500		1500		0				
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error.csv			PARTIALLY_LOADED	1500			1498		1500		2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS"["PROFIT":3]  
  
insert into rejected_order_records  
select current_date() as run_date, current_timestamp() as run_ts, t.* from table(validate(orders, job_id => '_last')) as t;

select * from copy_db.public.rejected_order_records;
RUN_DATE  |RUN_TS                 |ERROR                                               |FILE                                       |LINE|CHARACTER|BYTE_OFFSET|CATEGORY  |CODE  |SQL_STATE|COLUMN_NAME           |ROW_NUMBER|ROW_START_LINE|REJECTED_RECORD                                 |
----------+-----------------------+----------------------------------------------------+-------------------------------------------+----+---------+-----------+----------+------+---------+----------------------+----------+--------------+------------------------------------------------+
2024-06-25|2024-06-26 01:27:55.743|Numeric value '7-' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   2|       17|         71|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         1|             2|B-30601,1275,10,7-,Furniture,BookcasesÂ¶         |
2024-06-25|2024-06-26 01:27:55.743|Numeric value '3a' is not recognized                |returnfailed/OrderDetails_error2 - Copy.csv|   4|       16|        143|conversion|100038|22018    |"ORDERS"["QUANTITY":4]|         3|             4|B-30601,8,-244,3a,Clothing,HankerchiefÂ¶         |
2024-06-25|2024-06-26 01:27:55.743|Numeric value 'one thousand' is not recognized      |returnfailed/OrderDetails_error.csv        |   2|       14|         68|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         1|             2|B-25601,1275,one thousand,7,Furniture,BookcasesÂ¶|
2024-06-25|2024-06-26 01:27:55.743|Numeric value 'two hundred twenty' is not recognized|returnfailed/OrderDetails_error.csv        |   3|       12|        115|conversion|100038|22018    |"ORDERS"["PROFIT":3]  |         2|             3|B-25601,66,two hundred twenty,5,Clothing,StoleÂ¶ |

==========
SIZE_LIMIT
==========
Example for size_limit option

CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
	
name                                            |size |md5                             |last_modified                |
------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/size/Orders.csv |54600|1a1c4a47a8e8e43ecef5bf8a46ee4017|Wed, 28 Apr 2021 16:14:06 GMT|
s3://snowflakebucket-copyoption/size/Orders2.csv|54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:29:32 GMT|

here both the file having size more than 50k bytes, the below example allow to load upto 20k bytes. Lets see what happens

truncate table COPY_DB.PUBLIC.ORDERS;

//Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    SIZE_LIMIT=20000;

file											status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflakebucket-copyoption/size/Orders.csv	LOADED	1500		1500		1			0				

Though the contains morethan 50k bytes, snowflake process the current file and load the file fully and reject the remaining files once the maximum size threshold is reached. This is to avoid 
partial loading of a file.

even if the size_limit is 10 bytes, it will load the file as it picked it for processing. Assume if the size_limit is 60k then it loads both the files. Since, the
first file has 54600 bytes which is less than the size_limit so it will pick the next availale file and start processing it and fully load the data even the cumulative
file size reaches the size_limit threshold.

==================
RETURN_FAILED_ONLY
==================

RETURN_FAILED_ONLY = True / False
If we set the value as 'True' in the copy into command, this option loads the data if there is no error, incase of errors will return the information about the 
errored files only. 
Default value is false. 

If we set this flag = 'True' along with ON_ERROR = 'CONTINUE' then the copy into command will load the valid records into the table and return the failed records in the display 
console.


// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/returnfailed/';
  
LIST @COPY_DB.PUBLIC.aws_stage_copy
name                                                                       |size |md5                             |last_modified                |
---------------------------------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error.csv        |54622|99bb5d5b87e74256ca04c91359204dba|Wed, 28 Apr 2021 16:36:24 GMT|
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error2 - Copy.csv|10514|7c9dc0c0c8e6a9b82173c3df091a746a|Thu, 29 Apr 2021 08:02:50 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders.csv                    |54597|cf56c5cfede468cdcacac48d7ab67e75|Wed, 28 Apr 2021 16:36:06 GMT|
s3://snowflakebucket-copyoption/returnfailed/Orders2.csv                   |54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:36:07 GMT|

truncate table COPY_DB.PUBLIC.ORDERS;
    
 //Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    RETURN_FAILED_ONLY = TRUE ;

File 'returnfailed/OrderDetails_error2 - Copy.csv', line 2, character 17
Row 1, column "ORDERS"["QUANTITY":4]
If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. 
For more information on loading options, please run 'info loading_data' in a SQL client.  
  
//Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
	on_error = 'continue'
    RETURN_FAILED_ONLY = TRUE ;

file																		file_count	status				rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
-																			2			LOADED				3000		3000		0				
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error2 - Copy.csv	1			PARTIALLY_LOADED	285			283			285			2			Numeric value '7-' is not recognized	2	17	"ORDERS"["QUANTITY":4]
s3://snowflakebucket-copyoption/returnfailed/OrderDetails_error.csv			1			PARTIALLY_LOADED	1500		1498		1500		2			Numeric value 'one thousand' is not recognized	2	14	"ORDERS"["PROFIT":3]

in this example two files were loaded successfully but file informations are not shown.

===============
TRUNCATECOLUMNS
===============

TRUNCATECOLUMNS = True/False

if this parameter is set to true, in case if the value given in the input file is more than the field length of the CHAR/VARCHAR/STRING data type fields then 
it will autometically truncate the excess characters and load the data into the tables. Default value is False.

CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
  
LIST @COPY_DB.PUBLIC.aws_stage_copy
 
name                                            |size |md5                             |last_modified                |
------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/size/Orders.csv |54600|1a1c4a47a8e8e43ecef5bf8a46ee4017|Wed, 28 Apr 2021 16:14:06 GMT|
s3://snowflakebucket-copyoption/size/Orders2.csv|54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:29:32 GMT|

truncate table copy_db.public.orders;

recreate the table copy_db.public.orders and reduce the field length for the column category from 30 to 10.

CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(10),
    SUBCATEGORY VARCHAR(30));


// this will fail due to the excess field length
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
	
SQL Error [100074] [54000]: User character length limit (10) exceeded by string 'Electronics'
  File 'size/Orders2.csv', line 5, character 18
  Row 4, column "ORDERS"["CATEGORY":5]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. 
  For more information on loading options, please run 'info loading_data' in a SQL client.

// this will fail due to the excess field length
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
	truncatecolumns = true
    pattern='.*Order.*';
	
file												status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflakebucket-copyoption/size/Orders.csv		LOADED	1500		1500		1			0				
s3://snowflakebucket-copyoption/size/Orders2.csv	LOADED	1500		1500		1			0				

=====
FORCE
=====

Force = True / False

Generally once the file is loaded, if we execute the copy command again the same file will not get loaded. Snowflake maintains the metadata about the file
load_history and the file loaded successfully and will not allow to populate the file again to avoid the duplicate data. If there is a scenario to reload the 
file again then set the flag force = True, which will reload the file again and there will be duplicate data in the table.

CREATE OR REPLACE TABLE  COPY_DB.PUBLIC.ORDERS (
    ORDER_ID VARCHAR(30),
    AMOUNT VARCHAR(30),
    PROFIT INT,
    QUANTITY INT,
    CATEGORY VARCHAR(30),
    SUBCATEGORY VARCHAR(30));

// Prepare stage object
CREATE OR REPLACE STAGE COPY_DB.PUBLIC.aws_stage_copy
    url='s3://snowflakebucket-copyoption/size/';
  
LIST @COPY_DB.PUBLIC.aws_stage_copy;
name                                            |size |md5                             |last_modified                |
------------------------------------------------+-----+--------------------------------+-----------------------------+
s3://snowflakebucket-copyoption/size/Orders.csv |54600|1a1c4a47a8e8e43ecef5bf8a46ee4017|Wed, 28 Apr 2021 16:14:06 GMT|
s3://snowflakebucket-copyoption/size/Orders2.csv|54598|36bcccace29563ceb1f86a62f599dba3|Wed, 28 Apr 2021 16:29:32 GMT|
    
//Load data using copy command
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
	
file													status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflakebucket-copyoption/size/Orders.csv			LOADED	1500	1	500			1			0				
s3://snowflakebucket-copyoption/size/Orders2.csv		LOADED	1500		1500		1			0				
	
select count(1) from COPY_DB.PUBLIC.ORDERS;
COUNT(1)|
--------+
    3000|

// Not possible to load file that have been loaded and data has not been modified
COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*';
	
status
Copy executed with 0 files processed.

select count(1) from COPY_DB.PUBLIC.ORDERS;
COUNT(1)|
--------+
    3000|

// Now load the data using the FORCE option

COPY INTO COPY_DB.PUBLIC.ORDERS
    FROM @aws_stage_copy
    file_format= (type = csv field_delimiter=',' skip_header=1)
    pattern='.*Order.*'
    FORCE = TRUE;
	
file													status	rows_parsed	rows_loaded	error_limit	errors_seen	first_error	first_error_line	first_error_character	first_error_column_name
s3://snowflakebucket-copyoption/size/Orders.csv			LOADED	1500	1	500			1			0				
s3://snowflakebucket-copyoption/size/Orders2.csv		LOADED	1500		1500		1			0		

select count(1) from COPY_DB.PUBLIC.ORDERS;
COUNT(1)|
--------+
    6000|

================================================================================================================================
End of Topic - copy options in detail
================================================================================================================================
