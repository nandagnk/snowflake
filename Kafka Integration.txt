/*
Kafka Integration
=================
Note: View the document in Notepad++
*/

Kafka is used for real time streaming. Example Whatapps messages, Chatbot, NetFlix, Live Cricket etc.
Producer/Publisher generate the streaming messages.
Consumer/Subscriber receives streaming messages.
Kafka server act as a Broker between the publisher and consumer.

Setting up Kafka
================
========================
Step 1 setting up Kafka
========================
To setup Kafka download the kafka software from Apache Kafka official website using the below link.
https://archive.apache.org/dist/kafka/3.2.1/kafka_2.13-3.2.1.tgz

Unzip the file using 7zip you will get a tar gile again unzip the tar file using 7zip.

you will see the following folders.
bin      ---will contains the executables 
config   ---will contain the configuration files 
libs     ---containse the jar files used to connect to any third party system like python, scala, snowflake etc
licences
site-docs

Create a folder Kafka in C-Drive and paste all the above folders and files into it.

Create a folder Kafka-logs under the folder c:/Kafka and create a sub-folders zookeeeper and server_logs

Directory of C:\Kafka

06/02/2024  07:55 PM    <DIR>          .
06/02/2024  07:55 PM    <DIR>          ..
06/02/2024  07:48 PM    <DIR>          bin
06/02/2024  07:48 PM    <DIR>          config
06/02/2024  07:55 PM    <DIR>          Kafka-logs
06/02/2024  07:48 PM    <DIR>          libs
07/22/2022  10:33 AM            14,640 LICENSE
06/02/2024  07:48 PM    <DIR>          licenses
07/22/2022  10:33 AM            28,184 NOTICE
06/02/2024  07:48 PM    <DIR>          site-docs
               2 File(s)         42,824 bytes
               8 Dir(s)  81,990,303,744 bytes free

Directory of C:\Kafka\Kafka-logs

06/02/2024  07:55 PM    <DIR>          .
06/02/2024  07:55 PM    <DIR>          ..
06/02/2024  07:55 PM    <DIR>          server_logs
06/02/2024  07:55 PM    <DIR>          zookeeper
               0 File(s)              0 bytes
               4 Dir(s)  81,989,767,168 bytes free
==================================
Step 2 change zookeeper.properties
==================================
Now open the file "zookeeper.properties" in the folder C:\Kafka\config
and update the following properties

dataDir=c:/Kafka/Kafka_logs/zookeeper
This property defines where the zookeeper logs to be captured

maxClientCnxns=1
This property limits the number of active connections from a host, specified by IP address, to a single ZooKeeper server.

===============================
Step 3 change server.properties
===============================
Now open the file "server.properties" in the folder c:\Kafka\config
and update the following properties.

log.dirs=C:/Kafka/Kafka-logs/server_logs
zookeeper.connect=localhost:2181   --This should match the port number given in the zookeeeper.properties file.
zookeeper.connection.timeout.ms=6000000   --default 18000 if the broker is not responding within this time it consider that timedout.

===============================
Step 4 start zookeeper service.
===============================
Now start the zookeeper service.

connect to command prompt as administrator mode and 

Navigate to C:\kafka\bin\windows in windows and type cmd(it will open command prompt with the C:\kafka\bin\windows path)

zookeeper-server-start.bat C:\kafka\config\zookeeper.properties

executing the zookeeper-server-start.bat by passing the zookeeper.properties file location as parameter. This will start the zookeeeper service.

C:\Kafka\bin\windows>zookeeper-server-start.bat C:\kafka\config\zookeeper.properties
[2024-06-02 20:10:33,604] INFO Reading configuration from: C:\kafka\config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2024-06-02 20:10:33,616] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2024-06-02 20:10:33,616] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2024-06-02 20:10:33,616] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2024-06-02 20:10:33,616] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)

[2024-06-02 20:10:33,700] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,700] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,701] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,702] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,703] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,703] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,704] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,704] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,705] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,705] INFO  (org.apache.zookeeper.server.ZooKeeperServer)

[2024-06-02 20:10:33,776] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2024-06-02 20:10:33,776] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
[2024-06-02 20:10:33,785] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
[2024-06-02 20:10:33,785] INFO Snapshotting: 0x0 to c:\Kafka\Kafka_logs\zookeeper\version-2\snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2024-06-02 20:10:33,795] INFO Snapshot loaded in 17 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
[2024-06-02 20:10:33,795] INFO Snapshotting: 0x0 to c:\Kafka\Kafka_logs\zookeeper\version-2\snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2024-06-02 20:10:33,795] INFO Snapshot taken in 1 ms (org.apache.zookeeper.server.ZooKeeperServer)
[2024-06-02 20:10:33,820] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
[2024-06-02 20:10:33,821] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
[2024-06-02 20:10:33,838] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
[2024-06-02 20:10:33,839] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)

*************************************************
*********Note: don't close this window***********
*************************************************

==========================
Step 5 start Kafka server
==========================
open a command prompt window in administrator mode and execute the below command. This command executes the kafka-server-start.bat batch file
by passing the server.properties file location as input.

C:\kafka\bin\windows\kafka-server-start.bat C:\kafka\config\server.properties

C:\WINDOWS\system32>C:\kafka\bin\windows\kafka-server-start.bat C:\kafka\config\server.properties
[2024-06-02 20:22:21,075] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2024-06-02 20:22:21,365] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2024-06-02 20:22:21,462] INFO starting (kafka.server.KafkaServer)
[2024-06-02 20:22:21,463] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2024-06-02 20:22:21,478] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2024-06-02 20:22:21,486] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
[2024-06-02 20:22:21,486] INFO Client environment:host.name=DESKTOP-E4HB1LH (org.apache.zookeeper.ZooKeeper)
[2024-06-02 20:22:21,486] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
[2024-06-02 20:22:21,486] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)

[2024-06-02 20:22:21,512] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)
[2024-06-02 20:22:21,515] INFO Socket connection established, initiating session, client: /127.0.0.1:56371, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2024-06-02 20:22:21,536] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x1005b79692b0000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[2024-06-02 20:22:21,539] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2024-06-02 20:22:21,663] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
[2024-06-02 20:22:21,675] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
[2024-06-02 20:22:21,676] INFO Cleared cache (kafka.server.FinalizedFeatureCache)
[2024-06-02 20:22:21,828] INFO Cluster ID = hKKX9Uz8Sgim7hTK2P182g (kafka.server.KafkaServer)
[2024-06-02 20:22:21,832] WARN No meta.properties file under dir C:\Kafka\Kafka-logs\server_logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2024-06-02 20:22:21,887] INFO KafkaConfig values:
        advertised.listeners = null
        alter.config.policy.class.name = null
        alter.log.dirs.replication.quota.window.num = 11
        alter.log.dirs.replication.quota.window.size.seconds = 1
        authorizer.class.name =
        auto.create.topics.enable = true
        auto.leader.rebalance.enable = true
        background.threads = 10
        broker.heartbeat.interval.ms = 2000
        broker.id = 0
        broker.id.generation.enable = true
        broker.rack = null
        broker.session.timeout.ms = 9000
        client.quota.callback.class = null
        compression.type = producer
        connection.failed.authentication.delay.ms = 100
        connections.max.idle.ms = 600000
		
[2024-06-02 20:22:22,723] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2024-06-02 20:22:22,735] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache)
[2024-06-02 20:22:22,742] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2024-06-02 20:22:22,749] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2024-06-02 20:22:22,795] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2024-06-02 20:22:22,855] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer)
[2024-06-02 20:22:22,876] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2024-06-02 20:22:22,879] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer)
[2024-06-02 20:22:22,881] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2024-06-02 20:22:22,923] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use broker DESKTOP-E4HB1LH:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2024-06-02 20:22:22,923] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker DESKTOP-E4HB1LH:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2024-06-02 20:22:22,933] INFO Kafka version: 3.2.1 (org.apache.kafka.common.utils.AppInfoParser)
[2024-06-02 20:22:22,933] INFO Kafka commitId: b172a0a94f4ebb9f (org.apache.kafka.common.utils.AppInfoParser)
[2024-06-02 20:22:22,934] INFO Kafka startTimeMs: 1717323742889 (org.apache.kafka.common.utils.AppInfoParser)
[2024-06-02 20:22:22,935] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)

***********************************************************************************************
Notedown the Ip of the PLAINTEXT, this is required for creating a topic
[2024-06-02 20:22:22,572] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://DESKTOP-E4HB1LH:9092, 
czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)
***********************************************************************************************

************************************************
************don't close the server**************
************************************************

=========================
Step 6 Create a Topic
=========================
Open a new command prompt window in adminstrator mode and exeucte the following, not the ip address should refer the PLAINTEXT ip address from the 
kafka server. In this example the IP address is PLAINTEXT://DESKTOP-E4HB1LH:9092, use this while creating the topic.

C:\kafka\bin\windows\kafka-topics.bat --create --topic mytopic1 --bootstrap-server DESKTOP-E4HB1LH:9092 --replication-factor 1 --partitions 1

C:\WINDOWS\system32>C:\kafka\bin\windows\kafka-topics.bat --create --topic mytopic1 --bootstrap-server DESKTOP-E4HB1LH:9092 --replication-factor 1 --partitions 1
Created topic mytopic1.

==========================
Step 7 Start the Producer
==========================

Open a new command prompt window in adminstrator mode and execute the below command, make sure the IP is referred correctly from the 
Kafka Server PLAINTEXT, this will prompt to key in the messages.

C:\kafka\bin\windows\kafka-console-producer.bat --topic mytopic --bootstrap-server DESKTOP-E4HB1LH:9092


C:\WINDOWS\system32>C:\kafka\bin\windows\kafka-console-producer.bat --topic mytopic --bootstrap-server DESKTOP-E4HB1LH:9092
>

Note: Don't close the window

==========================
Step 8 Start the Consumer
==========================

Open a new command prompt window in administrator mode and execute the below command,

C:\kafka\bin\windows\kafka-console-consumer.bat --topic mytopic --from-beginning --bootstrap-server DESKTOP-E4HB1LH:9092

C:\WINDOWS\system32>C:\kafka\bin\windows\kafka-console-consumer.bat --topic mytopic --from-beginning --bootstrap-server DESKTOP-E4HB1LH:9092
[2024-06-02 21:14:00,782] WARN [Consumer clientId=console-consumer, groupId=console-consumer-89666] Error while fetching metadata with correlation id 2 : {mytopic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)

when we start key in messages in the producer, they will appear in the consumer.

resize the producer window and consumer window and keep then near and start key in text in the producer window and check whether they are 
coming in the consumer window.

The following were keyed in the producer window
C:\WINDOWS\system32>C:\kafka\bin\windows\kafka-console-producer.bat --topic mytopic --bootstrap-server DESKTOP-E4HB1LH:9092
>Hi
>Kafka streaming testing
>1
>2
>3
>4
>5
>6
>


And the following were received in the consumer window.

[2024-06-02 21:14:00,782] WARN [Consumer clientId=console-consumer, groupId=console-consumer-89666] Error while fetching metadata with correlation id 2 : {mytopic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
Hi
Kafka streaming testing
1
2
3
4
5
6

====================================================================================================================
So far we tested the kafka streaming by sending the topics and receiving the messages from command window, but in real time the producer
will be the application/Webpage/API/any tool which will send the topic messages and the consumer will be a DWH tool, or big data or snowflake or any 
other application
====================================================================================================================

Make a python program as a producer and send topics to the broker (Kafka server) and consume the message from the snowflake DB.
==================================
Step 1 Make Python as a producer.
==================================
To make the python to interact with the Kafka server install the kafka library in Python.

Open a new command prompt window and execute the command "pip install kafka-python"

C:\Users\Windows 10>python -m pip install kafka-python
Collecting kafka-python
  Obtaining dependency information for kafka-python from https://files.pythonhosted.org/packages/75/68/dcb0db055309f680ab2931a3eeb22d865604b638acf8c914bedf4c1a0c8c/kafka_python-2.0.2-py2.py3-none-any.whl.metadata
  Downloading kafka_python-2.0.2-py2.py3-none-any.whl.metadata (7.8 kB)
Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)
   ---------------------------------------- 246.5/246.5 kB 1.9 MB/s eta 0:00:00
Installing collected packages: kafka-python
Successfully installed kafka-python-2.0.2

[notice] A new release of pip is available: 23.2.1 -> 24.0
[notice] To update, run: python.exe -m pip install --upgrade pip

C:\Users\Windows 10>

Now, open vs code and type the following python code., 

from time import sleep
from json import dumps
from kafka import KafkaProducer

topic_name='mytopic'
producer = KafkaProducer(bootstrap_servers=['DESKTOP-E4HB1LH:9092'],value_serializer=lambda x: dumps(x).encode('utf-8'))

for e in range(100):
    data = {'number' : e}
    print(data)
    producer.send(topic_name, value=data)
    sleep(3)
	
if there are any issue, go to the terminal in the VS CODE and enter the following command.

pip install kafka-python
        ^^^^^^^
Downloading kafka_python-2.0.2-py2.py3-none-any.whl 
(246 kB)                                             kafka-python
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 246.5/246.5 kB 2.5 MB/s eta 0:00:00                         l.metadata (7.8 kB)
Installing collected packages: kafka-python         (246 kB)
Successfully installed kafka-python-2.0.2           6.5 kB 2.5 MB/s eta 0:00:00
PS C:\Users\Windows 10\vscode> & "c:/Users/Windows 10/vscode/.venv/Scripts/python.exe" "c:/Users/Windows 10/vscode/test1.py"                                0/vscode/.venv/Scripts/python.exe" "c:/Users/Windows 10/vscode/test1.py"

Now run the program and Expected output is 
{'number': 3}
{'number': 4}
{'number': 5}
{'number': 6}
{'number': 7}
{'number': 8}
{'number': 9}
{'number': 10}
{'number': 11}
{'number': 12}

Now open the consumer window and check whether the consumer started receiving this messages.

orkClient)
Hi
Kafka streaming testing
1
2
3
4
5
6
{"number": 0}
{"number": 1}
{"number": 2}
{"number": 3}
{"number": 4}
{"number": 5}
{"number": 6}
{"number": 7}
{"number": 8}
{"number": 9}
{"number": 10}
{"number": 11}
{"number": 12}
{"number": 13}
{"number": 14}
{"number": 15}
{"number": 16}
{"number": 17}
{"number": 18}
{"number": 19}
{"number": 20}

===================================
Step 2 Make Snowflake as a consumer
===================================

**********************************************************************
#################### Kafka Snowflake Integration: ####################
**********************************************************************
Download the required jar file --> https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector

Put this jar in libs folder in kafka folder

Update the plugin.path in kafka connect-standalone properties.

plugin.path=C:/kafka/libs

As I'm running this in my local maching hence updating plugin.path property in "connect-standalone.properties" file in the config folder.\
In case of real time when we are going to use this from a distributed system then update the property in the file "connect-distributed.properties".

------------------------------------
Create Private & Public key-pair:
------------------------------------
refer the below snowflake document
https://docs.snowflake.com/en/user-guide/kafka-connector-install#label-kafka-key-pair-authn-rotation

Download and install openssl if not installed from below link and set the environment variable path
https://slproweb.com/products/Win32OpenSSL.html

To generate an unencrypted version, use the following command:
openssl genrsa -out rsa_key.pem 2048

C:\Users\Windows 10>openssl genrsa -out rsa_key.pem 2048

This will create the file rsa_key.pem in the current folder "C:\Users\Windows 10"

To generate the public key for the private key generated in the above step exeute the below command(check the file location "rsa_key.pem" 
and change the location accordingly)
openssl rsa -in C:\Users\Windows 10\rsa_key.pem -pubout -out rsa_key.pub

C:\Users\Windows 10>openssl rsa -in "C:\Users\Windows 10\rsa_key.pem" -pubout -out rsa_key.pub
writing RSA key

now both the pem and pub files are generated.
C:\Users\Windows 10>dir rsa*.*
 Volume in drive C has no label.
 Volume Serial Number is 4222-1ECC

 Directory of C:\Users\Windows 10

06/02/2024  10:19 PM             1,732 rsa_key.pem
06/02/2024  10:25 PM               460 rsa_key.pub
               2 File(s)          2,192 bytes
               0 Dir(s)  79,322,759,168 bytes free
			   
Configure the Public key in Snowflake account for the active user.

Configure the public key in Snowflake:
----------------------------------------------------------------

alter user {User_name} set rsa_public_key='{Put the Public key content here}';

login to Snowflake account, change role to accountadmin and execute the alter statement.

use ROLE accountadmin;
select current_role(),current_user();
CURRENT_ROLE()|CURRENT_USER()|
--------------+--------------+
ACCOUNTADMIN  |GNANDA80      |

alter user gnanda80 set rsa_public_key='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAy+JTxLsxQq2m2sEeHbs+
2JzvftxyPDNsUIxqIOFBb8oNhCkEuMvBWdKu4Y+CEHK9WUcH2BEByPrvpjGQ0fh1
omPQzOV06iLdVGqKpbT21JavFDZJrx2cVhOCFRtqyLZMfAAB2+xmvltdcL+PBowN
kclE1Kwvo/JI7PkbJLx6L4B3LvsH9MeQf0Z444Lyc4BKaW/n3HwVKeRs4cZrg/Lc
WTv5e12Eydj9ooy4EX31WBllK1ojh83qtDO9pNKvFKR8g8dhqegsn/CbODZC4whq
KLALmxKhHNWxJte3h4muZqUs/2XPljnUHW2cKZHwl2h/oOGZlBtqJ0RLLcsHkXA7
lwIDAQAB';

desc user gnanda80;
property                                  |value               
------------------------------------------+--------------------
NAME                                      |GNANDA80            
COMMENT                                   |null                
DISPLAY_NAME                              |GNANDA80            
LOGIN_NAME                                |GNANDA80            
FIRST_NAME                                |Nanda               
MIDDLE_NAME                               |null                
LAST_NAME                                 |Govindan            
EMAIL                                     |xxxxxxxxxxxxxx@gmail.com 
PASSWORD                                  |********            
MUST_CHANGE_PASSWORD                      |false               
DISABLED                                  |false               
SNOWFLAKE_LOCK                            |false               
SNOWFLAKE_SUPPORT                         |false               
DAYS_TO_EXPIRY                            |null                
MINS_TO_UNLOCK                            |null                
DEFAULT_WAREHOUSE                         |COMPUTE_WH          
DEFAULT_NAMESPACE                         |null                
DEFAULT_ROLE                              |ACCOUNTADMIN        
DEFAULT_SECONDARY_ROLES                   |null                
EXT_AUTHN_DUO                             |false               
EXT_AUTHN_UID                             |null                
MINS_TO_BYPASS_MFA                        |null                
MINS_TO_BYPASS_NETWORK_POLICY             |null                
RSA_PUBLIC_KEY                            |MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAy+JTxLsxQq2m2sEeHbs+
2JzvftxyPDNsUIxqIOFBb8oNhCkEuMvBWdKu4Y+CEHK9WUcH2BEByPrvpjGQ0fh1
omPQzOV06iLdVGqKpbT21JavFDZJrx2cVhOCFRtqyLZMfAAB2+xmvltdcL+PBowN
kclE1Kwvo/JI7PkbJLx6L4B3LvsH9MeQf0Z444Lyc4BKaW/n3HwVKeRs4cZrg/Lc
WTv5e12Eydj9ooy4EX31WBllK1ojh83qtDO9pNKvFKR8g8dhqegsn/CbODZC4whq
KLALmxKhHNWxJte3h4muZqUs/2XPljnUHW2cKZHwl2h/oOGZlBtqJ0RLLcsHkXA7
lwIDAQAB
RSA_PUBLIC_KEY_FP                         |SHA256:3p5gd1EiNDUPM
RSA_PUBLIC_KEY_2                          |null                
RSA_PUBLIC_KEY_2_FP                       |null                
PASSWORD_LAST_SET_TIME                    |2024-05-15 01:09:31.
CUSTOM_LANDING_PAGE_URL                   |null                
CUSTOM_LANDING_PAGE_URL_FLUSH_NEXT_UI_LOAD|false         


now, create snowflake connect properties file.

https://docs.snowflake.com/en/user-guide/kafka-connector-install#label-kafka-connector-configuration-file

Create a SF_connect.properties file with below properties in config folder --

connector.class=com.snowflake.kafka.connector.SnowflakeSinkConnector
tasks.max=8
topics={topic_name}
snowflake.topic2table.map={topic_name}:{snowflake_table_name}
buffer.count.records=10000
buffer.flush.time=60
buffer.size.bytes=5000000
snowflake.url.name={Snowflake URL}
snowflake.user.name={Snowflake User Name}
snowflake.private.key={Put the Private key content here}
snowflake.database.name={Snowflake Database Name}
snowflake.schema.name={Snowflake Schema Name}
key.converter=com.snowflake.kafka.connector.records.SnowflakeJsonConverter
value.converter=com.snowflake.kafka.connector.records.SnowflakeJsonConverter
name={}

update the property snowflake.private.key with the private key generated in the earlier steps. make sure newline characters are 
replaced with a '\' character. And finally place the file "snowflake_connect.properties" in the config folder.

And start the kafka connecter service.

Open a new command prompt window in adminstrar mode and execute the command

C:\kafka\bin\windows\connect-standalone.bat C:\kafka\config\connect-standalone.properties C:\kafka\config\snowflake_connect.properties

now login to snowflake and check whether the table kafka_raw and stages are created.

use ROLE accountadmin;
use kafka_streaming_db.kafka_streaming_schema;
show tables;
created_on             |name     |database_name     |schema_name           |kind |comment|cluster_by|rows|bytes|owner       |retention_time|automatic_clustering|change_tracking|search_optimization|search_optimization_progress|search_optimization_bytes|is_external|enable_schema_evolution|owner_role_type|is_event|budget|is_hybrid|is_iceberg|is_dynamic|
-----------------------+---------+------------------+----------------------+-----+-------+----------+----+-----+------------+--------------+--------------------+---------------+-------------------+----------------------------+-------------------------+-----------+-----------------------+---------------+--------+------+---------+----------+----------+
2024-06-02 22:53:39.631|KAFKA_RAW|KAFKA_STREAMING_DB|KAFKA_STREAMING_SCHEMA|TABLE|       |          | 107| 3584|ACCOUNTADMIN|1             |OFF                 |OFF            |OFF                |                            |                         |N          |N                      |ROLE           |N       |      |N        |N         |N         |

now run the producer and check whether the data created in snowflake table.

SELECT record_content:number AS data_consumed 
FROM kafka_raw 
WHERE record_content:number IS NOT null;

DATA_CONSUMED|
-------------+
0            |
1            |
2            |
3            |
4            |
5            |
6            |
7            |
8            |
9            |
10           |
11           |
12           |
13           |
14           |
15           |
16           |
17           |
18           |
19           |
20           |
21           |
22           |
23           |
24           |
25           |
26           |
27           |
28           |
29           |
30           |
31           |
32           |
33           |
34           |
35           |
36           |
37           |
38           |
39           |
40           |
0            |
1            |
2            |
3            |
4            |
5            |
6            |
7            |
8            |
9            |
10           |
11           |
12           |
13           |
14           |
15           |
16           |
17           |
18           |
19           |
20           |
21           |
22           |
23           |
24           |
25           |

now create a dynamic table on top of kafka_raw table.

use ROLE accountadmin;
CREATE DYNAMIC TABLE dt_kafka_streaming 
target_lag = '1 minute'
warehouse=compute_wh
AS
SELECT record_content:number AS data_consumed 
FROM kafka_raw 
WHERE record_content:number IS NOT null;

SELECT count(1) FROM dt_kafka_streaming;
COUNT(1)|
--------+
     150|
	 
Run the producer one more time and see what happens

from time import sleep
from json import dumps
from kafka import KafkaProducer

topic_name='mytopic'
producer = KafkaProducer(bootstrap_servers=['DESKTOP-E4HB1LH:9092'],value_serializer=lambda x: dumps(x).encode('utf-8'))

for e in range(51,70,1):
    data = {'number' : e}
    print(data)
    producer.send(topic_name, value=data)
    sleep(3)
	
now it should add 20 numbers from 51 to 69 in the dynamic table.

SELECT current_timestamp() timestamp, count(1) rec_count FROM dt_kafka_streaming;
TIMESTAMP              |REC_COUNT|
-----------------------+---------+
2024-06-02 23:10:20.721|      150|

after few minutes it should reflect the data.

TIMESTAMP              |REC_COUNT|
-----------------------+---------+
2024-06-02 23:11:03.851|      151|
reexecute the query again in different interval (within few seconds/minutes)
TIMESTAMP              |REC_COUNT|
-----------------------+---------+
2024-06-02 23:11:50.371|      169|

Now, the POC is success, hence, disable the dynamic table to stop consuming credits.

alter dynamic table dt_kafka_streaming suspend;
status                          |
--------------------------------+
Statement executed successfully.|

close all the windows (Producer, consumer, server and zookeeper).

Note: Clears the logs in the zookeeeper and server_logs before starting them again.